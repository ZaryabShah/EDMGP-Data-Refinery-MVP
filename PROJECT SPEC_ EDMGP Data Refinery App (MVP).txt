PROJECT SPEC: EDMGP Data Refinery App (MVP)
Goal: Build a local, desktop-based Streamlit application to ingest, validate, slice, and normalize audio/MIDI datasets for Machine Learning training.
Core Philosophy:
1. Data Integrity: The MIDI file is the "Ground Truth" for timing. Audio must align perfectly with MIDI.
2. No Typos: The user should never manually type a filename. All naming is programmatic based on UI selections.
3. Local Execution: No cloud dependencies for V1. Everything runs on the local file system.
1. THE WORKFLOW (User Journey)
Step 1: Ingest & Initial Filtering
* Input: User selects a local Source Directory.
* The "Vocal Rights" Gate:
   * Add a Toggle/Checkbox: "Contains Exclusive Vocals?"
   * Logic: If FALSE (Royalty-Free/Splice Vocals), the app must Discard/Ignore any vocal stems found in the folder. They cannot enter the dataset.
Step 2: Auto-Pairing (Conditional)
* Logic: Scan for .wav and .mid files. Use Fuzzy Matching (Levenshtein) to pair them.
* Grouping Rules:
   * Melodic (Bass, Synth, Instruments): MIDI pairing is REQUIRED. (Show warning if missing).
   * Non-Melodic (Drums, FX): MIDI pairing is OPTIONAL.
Step 3: The "Slicer" Interface
* Visualizer: Display Audio Waveform.
* Grid: Overlay Beat Grid derived from MIDI Tempo Map (if available) or Audio Transient Detection (if no MIDI).
* Action: User selects Start/End points.
* Crop: App slices Audio (and MIDI if present) to the exact same length.
Step 4: Labeling & Validation
* Taxonomy (Dropdowns):
   * Group: Drums, Bass, Synth, Vocal, FX
   * Instrument: (e.g., Kick, Sub, Lead, Pad)
   * Layer: Main, High, Low, Texture
* Processing Logic (Stereo vs. Mono):
   * Force MONO if: Instrument is Kick, Snare (Main), Sub Bass, or Lead Vocal.
   * Keep STEREO if: Group is FX or Instrument is Pad, Ambience, Cymbal.
Step 5: Export
* Rename: Apply schema: [UID]_[Group]_[Instrument]_[Layer].wav
* Sidecar: Generate metadata.json.
* Output: Save to /Clean_Dataset_Staging/
2. DATA SCHEMA
A. Directory Structure & Naming Examples The app must output files that look exactly like this:
/Clean_Dataset_Staging
  ├── /Batch_2025-12-08
  │     ├── /GP_00001_TechHouse_126_Fmin
  │     │     ├── /Audio
  │     │     │     ├── GP_00001_drums_kick_main.wav       <-- (Mono)
  │     │     │     ├── GP_00001_bass_sub_main.wav         <-- (Mono)
  │     │     │     ├── GP_00001_synth_lead_layer1.wav     <-- (Stereo)
  │     │     │     └── GP_00001_fx_riser_texture.wav      <-- (Stereo)
  │     │     ├── /MIDI
  │     │     │     ├── GP_00001_midi_bass_sub.mid
  │     │     │     └── GP_00001_midi_synth_lead.mid
  │     │     └── metadata.json
B. Filename Construction Logic The filename is constructed programmatically based on the user's dropdown selections.
[UID]_[Group]_[Instrument]_[Layer].wav
Variable
	Source
	Examples
	UID
	Auto-Generated
	GP_00001, GP_00002
	Group
	Dropdown 1
	drums, bass, synth, vocal, fx
	Instrument
	Dropdown 2
	kick, snare, sub, lead, pad, atmosphere
	Layer
	Dropdown 3
	main, top, high, low, texture, double
	C. Metadata JSON Schema
{
  "uid": "GP_00001",
  "original_filename": "Kick_Final_v3.wav",
  "bpm": 126,
  "key": "Fmin",
  "tags": {
    "group": "drums",
    "instrument": "kick",
    "is_loop": false,
    "vocal_rights": "exclusive"  <-- (Critical Field)
  },
  "tech_specs": {
    "channels": 1,  <-- (1=Mono, 2=Stereo)
    "sample_rate": 44100
  }
}
3. TECH STACK
* Python 3.9+
* Streamlit
* Librosa (Audio Processing)
* Pretty_MIDI (MIDI Processing)
* RapidFuzz (String Matching)
4. DELIVERABLES (3-5 Days)
1. Source Code (GitHub/Zip)
2. Executable Script (run_app.py)
3. Demo Video: A 2-minute screen recording showing the tool processing one track successfully.
Note: I am running this on an M4 MacBook Pro (Apple Silicon). Please ensure the requirements.txt is compatible with ARM64 architecture so I don't run into build errors